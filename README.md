# EXP 5: Comparative Analysis of Different Types of Prompting Patterns
## Register Number: 212222230095

## Aim:

To test and compare how different prompt pattern models respond to naïve (broad/unstructured) prompts versus basic (clear/structured) prompts across multiple scenarios. The objective is to analyze the quality, accuracy, and depth of generated responses and observe how prompt clarity impacts the effectiveness of AI outputs.

## AI Tool Used:

ChatGPT (GPT-5)

Definition of Prompt Types:

### Naïve Prompt:

Simple, broad, and unstructured instruction.

Provides minimal context or constraints.

Example: “Tell me a story.”

Strength: Quick and flexible.
Weakness: Responses may be vague, generic, or off-target.

### Basic Prompt:

Clear, detailed, structured, and context-rich instruction.

Specifies constraints like length, tone, audience, and examples.

Example: “Write a 200-word fantasy story about a young girl who discovers a magical door in her village, include a twist ending.”

Strength: Produces accurate, relevant, and high-quality results.
Weakness: Requires more effort to craft.

## Test Scenarios

To compare prompt types, multiple tasks were chosen:

Creative story generation

Answering a factual question

Summarizing a concept

Providing career advice

Explaining a technical concept

For each scenario, a naïve prompt and a basic prompt were given, and responses were analyzed.

## Table of Comparison
| **Scenario**                  | **Naïve Prompt**            | **Basic Prompt**                                                                                                         | **Naïve Response (Summary)**                  | **Basic Response (Summary)**                                   | **Comparison**                                          |
| ----------------------------- | --------------------------- | ------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------- | -------------------------------------------------------------- | ------------------------------------------------------- |
| **1. Creative Story**         | “Tell me a story.”          | “Write a 200-word fantasy story about a young girl who discovers a magical door in her village, include a twist ending.” | Short, generic story, lacked depth.           | Well-structured story with strong characters, plot, and twist. | Basic prompt guided creativity → more immersive output. |
| **2. Factual Question**       | “What is AI?”               | “Explain Artificial Intelligence in simple terms within 4–5 sentences, with one real-world example.”                     | Technical definition, too formal.             | Clear, beginner-friendly, gave Siri/Google Maps example.       | Basic prompt improved clarity and relatability.         |
| **3. Summarization**          | “Summarize climate change.” | “Summarize climate change causes and effects in less than 100 words for school students.”                                | Lengthy, used technical terms.                | Concise, simple, explained causes & effects for students.      | Basic prompt ensured audience-specific accuracy.        |
| **4. Advice/Recommendation**  | “Give me career advice.”    | “Give me career advice for someone studying AI and Data Science who wants a job in software development.”                | General motivation (work hard, learn skills). | Targeted roadmap (DSA, projects, internships, interview prep). | Basic prompt delivered actionable, personalized advice. |
| **5. Explanation of Concept** | “Explain machine learning.” | “Explain machine learning in simple words, use an analogy, and limit to 5 sentences.”                                    | Technical explanation with jargon.            | Used analogy (“teaching a child”), very simple and short.      | Basic prompt increased depth + simplicity.              |


## Detailed Analysis

### Scenario 1 (Storytelling):

Naïve prompt led to a short, predictable story with little creativity.

Basic prompt forced the AI to construct a plot, character, and twist ending, leading to an engaging narrative.
Insight: AI performs better in creative writing when structure and constraints are given.

### Scenario 2 (Factual Question):

Naïve prompt gave a formal definition, suitable for experts but confusing for beginners.

Basic prompt added context (simple terms, length, example) → resulted in clear, audience-specific explanation.
Insight: For factual queries, clarity and examples enhance understanding.

### Scenario 3 (Summarization):

Naïve prompt output was too long, with unnecessary details.

Basic prompt created a short, targeted summary suited for students.
Insight: Summarization quality improves significantly when the target audience is specified.

### Scenario 4 (Career Advice):

Naïve prompt → generic “work hard, build skills” advice.

Basic prompt → career roadmap tailored for AI/Data Science students, including skills, tools, and placements strategy.
Insight: Advice becomes practical and actionable with structured prompts.

### Scenario 5 (Concept Explanation):

Naïve prompt used jargon-heavy explanation.

Basic prompt ensured simplicity + analogy, making it easy to grasp.
Insight: Adding analogies and constraints improves learning outcomes.

## Overall Observations:

Quality: Basic prompts consistently improved structure, creativity, and audience alignment.

Accuracy: Structured prompts reduced vagueness and improved factual correctness.

Depth: Naïve prompts often gave surface-level answers, while basic prompts added examples, analogies, and context.

Exception: In very simple tasks (e.g., “What is AI?”), naïve prompts gave correct answers but still lacked audience customization.

## Summary of Findings:

Prompt clarity directly impacts the quality of AI-generated responses.

Naïve prompts produce generic outputs; useful for brainstorming but not for precise results.

Basic prompts yield richer, more accurate, and more useful answers.

Best practices for prompting:

Be clear and specific.

Add constraints (length, tone, target audience).

Encourage depth using examples/analogies.

Always align the prompt with the end goal (creative vs factual vs explanatory).

## Result:

The experiment was successfully executed. ChatGPT produced better, more structured, and context-aware responses when provided with basic structured prompts compared to naïve prompts.





