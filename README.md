# EXP 5: Comparative Analysis of Different Types of Prompting Patterns
## Register Number: 212222230095

## Aim:

To test and compare how different prompt pattern models respond to naïve (broad/unstructured) prompts versus basic (clear/structured) prompts across multiple scenarios. The objective is to analyze the quality, accuracy, and depth of generated responses and observe how prompt clarity impacts the effectiveness of AI outputs.

## AI Tool Used:

ChatGPT (GPT-5)

Definition of Prompt Types:

### Naïve Prompt:

Simple, broad, and unstructured instruction.

Provides minimal context or constraints.

Example: “Tell me a story.”

Strength: Quick and flexible.
Weakness: Responses may be vague, generic, or off-target.

### Basic Prompt:

Clear, detailed, structured, and context-rich instruction.

Specifies constraints like length, tone, audience, and examples.

Example: “Write a 200-word fantasy story about a young girl who discovers a magical door in her village, include a twist ending.”

Strength: Produces accurate, relevant, and high-quality results.
Weakness: Requires more effort to craft.

## Test Scenarios

To compare prompt types, multiple tasks were chosen:

Creative story generation

Answering a factual question

Summarizing a concept

Providing career advice

Explaining a technical concept

For each scenario, a naïve prompt and a basic prompt were given, and responses were analyzed.

## Table of Comparison
Scenario	Naïve Prompt	Basic Prompt	Naïve Response (Summary)	Basic Response (Summary)	Comparison
1. Creative Story	“Tell me a story.”	“Write a 200-word fantasy story about a young girl who discovers a magical door in her village, include a twist ending.”	Short, generic story, lacked depth.	Well-structured story with strong characters, plot, and twist.	Basic prompt guided creativity → more immersive output.
2. Factual Question	“What is AI?”	“Explain Artificial Intelligence in simple terms within 4–5 sentences, with one real-world example.”	Technical definition, too formal.	Clear, beginner-friendly, gave Siri/Google Maps example.	Basic prompt improved clarity and relatability.
3. Summarization	“Summarize climate change.”	“Summarize climate change causes and effects in less than 100 words for school students.”	Lengthy, used technical terms.	Concise, simple, explained causes & effects for students.	Basic prompt ensured audience-specific accuracy.
4. Advice/Recommendation	“Give me career advice.”	“Give me career advice for someone studying AI and Data Science who wants a job in software development.”	General motivation (work hard, learn skills).	Targeted roadmap (DSA, projects, internships, interview prep).	Basic prompt delivered actionable, personalized advice.
5. Explanation of Concept	“Explain machine learning.”	“Explain machine learning in simple words, use an analogy, and limit to 5 sentences.”	Technical explanation with jargon.	Used analogy (“teaching a child”), very simple and short.	Basic prompt increased depth + simplicity.

## Detailed Analysis

### Scenario 1 (Storytelling):

Naïve prompt led to a short, predictable story with little creativity.

Basic prompt forced the AI to construct a plot, character, and twist ending, leading to an engaging narrative.
Insight: AI performs better in creative writing when structure and constraints are given.

### Scenario 2 (Factual Question):

Naïve prompt gave a formal definition, suitable for experts but confusing for beginners.

Basic prompt added context (simple terms, length, example) → resulted in clear, audience-specific explanation.
Insight: For factual queries, clarity and examples enhance understanding.

### Scenario 3 (Summarization):

Naïve prompt output was too long, with unnecessary details.

Basic prompt created a short, targeted summary suited for students.
Insight: Summarization quality improves significantly when the target audience is specified.

### Scenario 4 (Career Advice):

Naïve prompt → generic “work hard, build skills” advice.

Basic prompt → career roadmap tailored for AI/Data Science students, including skills, tools, and placements strategy.
Insight: Advice becomes practical and actionable with structured prompts.

### Scenario 5 (Concept Explanation):

Naïve prompt used jargon-heavy explanation.

Basic prompt ensured simplicity + analogy, making it easy to grasp.
Insight: Adding analogies and constraints improves learning outcomes.

## Overall Observations:

Quality: Basic prompts consistently improved structure, creativity, and audience alignment.

Accuracy: Structured prompts reduced vagueness and improved factual correctness.

Depth: Naïve prompts often gave surface-level answers, while basic prompts added examples, analogies, and context.

Exception: In very simple tasks (e.g., “What is AI?”), naïve prompts gave correct answers but still lacked audience customization.

## Summary of Findings:

Prompt clarity directly impacts the quality of AI-generated responses.

Naïve prompts produce generic outputs; useful for brainstorming but not for precise results.

Basic prompts yield richer, more accurate, and more useful answers.

Best practices for prompting:

Be clear and specific.

Add constraints (length, tone, target audience).

Encourage depth using examples/analogies.

Always align the prompt with the end goal (creative vs factual vs explanatory).

## Result:

The experiment was successfully executed. ChatGPT produced better, more structured, and context-aware responses when provided with basic structured prompts compared to naïve prompts.
